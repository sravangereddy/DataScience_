{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN CLASSIFIER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Table of contents\n",
    "    - Introduction to k-nearest neighbor classifier\n",
    "    - K- nearest neighbor classification step by step procedure.\n",
    "        - Knn applications<br>\n",
    "        - Parameters <br>\n",
    "    - How to choose the k value<br>\n",
    "    - K-Nearest Neighbor case study<br>\n",
    "    - Advantages of k-nearest neighbors algorithm<br>\n",
    "    - Limitations to k-nearest neighbors algorithm<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to k-nearest neighbor classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-Nearest Neighbors is one of the most basic yet essential classification algorithms in Machine Learning. It belongs to the supervised learning domain and finds intense application in pattern recognition, data mining and intrusion detection.\n",
    "\n",
    "It is widely disposable in real-life scenarios since it is non-parametric, meaning, it does not make any underlying assumptions about the distribution of data (as opposed to other algorithms such as GMM, which assume a Gaussian distribution of the given data).\n",
    "\n",
    "The simple version of the K-nearest neighbor classifier algorithms is to <b>predict the target label by finding the nearest neighbor class</b>. The closest class will be identified using the distance measures like Euclidean distance.\n",
    "\n",
    "<b>KNN is a non-parametric, lazy learning algorithm.</b>\n",
    "\n",
    "When we say a technique is <b>non-parametric , it means that it does not make any assumptions on the underlying data distribution</b>. In other words, the model structure is determined from the data. If you think about it, it’s pretty useful, because in the “real world”, most of the data does not obey the typical theoretical assumptions made (as in linear regression models, for example). \n",
    "\n",
    "KNN is also a <b>lazy algorithm</b> (as opposed to an eager algorithm). What this means is that it does not use the training data points to do any generalization. <b>In other words, there is no explicit training phase or it is very minimal.</b> This also means that the training phase is pretty fast . \n",
    "\n",
    "<i>Lack of generalization means that KNN keeps all the training data. To be more exact, all (or most) the training data is needed during the testing phase.</i>\n",
    "\n",
    "KNN Algorithm is based on <b>feature similarity: How closely out-of-sample features resemble our training set determines how we classify a given data point.</b>\n",
    "\n",
    "KNN can be used for classification — the output is a class membership (predicts a class — a discrete value). <b>An object is classified by a majority vote of its neighbors, with the object being assigned to the class most common among its k nearest neighbors. It can also be used for regression — output is the value for the object (predicts continuous values). This value is the average (or median) of the values of its k nearest neighbors.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Knn applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Credit ratings — collecting financial characteristics vs. comparing people with similar financial features to a database. By the very nature of a credit rating, people who have similar financial details would be given similar credit ratings. Therefore, they would like to be able to use this existing database to predict a new customer’s credit rating, without having to perform all the calculations.\n",
    "\n",
    "Should the bank give a loan to an individual? Would an individual default on his or her loan? Is that person closer in characteristics to people who defaulted or did not default on their loans?\n",
    "\n",
    "In political science — classing a potential voter to a “will vote” or “will not vote”, or to “vote Democrat” or “vote Republican”.\n",
    "\n",
    "More advance examples could include handwriting detection (like OCR), image recognition and even video recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to choose the k value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training error rate and the validation error rate are two parameters we need to access on different K-value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=10)\n",
    "train_val_df = X_train.merge(y_train,left_index=True,right_index=True)\n",
    "# train_val_df = fn_upsample(train_val_df_,target_)\n",
    "train_val_df.reset_index(inplace=True)\n",
    "train_val_df\n",
    "neighbours = np.arange(1,50,11)\n",
    "neighbours = np.append(neighbours,np.arange(51,200,11))\n",
    "precision_train=[]\n",
    "recall_train=[]\n",
    "precision_val=[]\n",
    "recall_val=[]\n",
    "for neighbour in neighbours:\n",
    "    knn_model = KNeighborsClassifier(n_neighbors=neighbour,p=1)\n",
    "    precision_train_=[]\n",
    "    recall_train_=[]\n",
    "    precision_val_=[]\n",
    "    recall_val_=[]\n",
    "    for train,val in kf.split(train_val_df):\n",
    "        knn_model.fit(train_val_df.loc[train,input_var],train_val_df.loc[train,target_])\n",
    "        \n",
    "        knn_predictions_train = knn_model.predict(train_val_df.loc[train,input_var])\n",
    "        knn_predictions_val = knn_model.predict(train_val_df.loc[val,input_var])\n",
    "        \n",
    "        knn_classificaiton_report_train,conufsion_matrix_train=fn_get_classification_report(train_val_df.loc[train,target_],\n",
    "                                                                                            knn_predictions_train)\n",
    "        knn_classificaiton_report_val,conufsion_matrix_val=fn_get_classification_report(train_val_df.loc[val,target_],\n",
    "                                                                                            knn_predictions_val)\n",
    "\n",
    "        metrics_train = fn_get_metrics(conufsion_matrix_train)\n",
    "        metrics_val = fn_get_metrics(conufsion_matrix_val)\n",
    "        \n",
    "        precision_train_.append(metrics_train['5 Precision'])\n",
    "        recall_train_.append(metrics_train['6 Recall'])\n",
    "        \n",
    "        precision_val_.append(metrics_val['5 Precision'])\n",
    "        recall_val_.append(metrics_val['6 Recall'])\n",
    "    \n",
    "    precision_train.append(metrics_train['5 Precision'])\n",
    "    recall_train.append(metrics_train['6 Recall'])\n",
    "        \n",
    "    precision_val.append(metrics_val['5 Precision'])\n",
    "    recall_val.append(metrics_val['6 Recall'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(13.5, 8.5))\n",
    "plt.plot(neighbours,precision_train,label='Train')\n",
    "plt.plot(neighbours,precision_val,label='Validation')\n",
    "plt.title('Learning Curve - Precison')\n",
    "plt.xlabel('Neighbours')\n",
    "plt.ylabel('Precision')\n",
    "plt.grid(linestyle='-', linewidth=0.5)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/LearningCurve.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After applying cross validation on both training and validaton datasets, plotting the precison for each value helps to choose the right K Value. In the above graph, at K=1, the training dataset has a precison of 1 where as on the validation set, the precision was close to 0. This is where our model has over fit, also known as, high variance. As K increases, the precision on validation data set too increases and at some point the value recahes a threshold. This value is the opitimum value of K. In the case, choosing a K value of 30 gives the right results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-NN Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class sklearn.neighbors.NearestNeighbors(n_neighbors=5, radius=1.0, algorithm=’auto’, leaf_size=30, metric=’minkowski’, p=2, metric_params=None, n_jobs=1, **kwargs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n_neighbors : int, optional (default = 5)\n",
    "\n",
    "Number of neighbors to use by default for kneighbors queries.\n",
    "\n",
    "radius : float, optional (default = 1.0)\n",
    "\n",
    "Range of parameter space to use by default for radius_neighbors queries.\n",
    "\n",
    "algorithm : {‘auto’, ‘ball_tree’, ‘kd_tree’, ‘brute’}, optional\n",
    "\n",
    "Algorithm used to compute the nearest neighbors:\n",
    "\n",
    "    ‘ball_tree’ will use BallTree\n",
    "    ‘kd_tree’ will use KDTree\n",
    "    ‘brute’ will use a brute-force search.\n",
    "    ‘auto’ will attempt to decide the most appropriate algorithm based on the values passed to fit method.\n",
    "    Note: fitting on sparse input will override the setting of this parameter, using brute force.\n",
    "\n",
    "leaf_size : int, optional (default = 30)\n",
    "\n",
    "    Leaf size passed to BallTree or KDTree. This can affect the speed of the construction and query, as well as the memory required to store the tree. The optimal value depends on the nature of the problem.\n",
    "\n",
    "metric : string or callable, default ‘minkowski’\n",
    "\n",
    "    metric to use for distance computation. Any metric from scikit-learn or scipy.spatial.distance can be used.\n",
    "\n",
    "    If metric is a callable function, it is called on each pair of instances (rows) and the resulting value recorded. The callable should take two arrays as input and return one value indicating the distance between them. This works for Scipy’s metrics, but is less efficient than passing the metric name as a string.\n",
    "\n",
    "    Distance matrices are not supported.\n",
    "\n",
    "    Valid values for metric are:\n",
    "\n",
    "    from scikit-learn: [‘cityblock’, ‘cosine’, ‘euclidean’, ‘l1’, ‘l2’, ‘manhattan’]\n",
    "    from scipy.spatial.distance: [‘braycurtis’, ‘canberra’, ‘chebyshev’, ‘correlation’, ‘dice’, ‘hamming’, ‘jaccard’, ‘kulsinski’, ‘mahalanobis’, ‘matching’, ‘minkowski’, ‘rogerstanimoto’, ‘russellrao’, ‘seuclidean’, ‘sokalmichener’, ‘sokalsneath’, ‘sqeuclidean’, ‘yule’]\n",
    "    See the documentation for scipy.spatial.distance for details on these metrics.\n",
    "\n",
    "p : integer, optional (default = 2)\n",
    "\n",
    "    Parameter for the Minkowski metric from sklearn.metrics.pairwise.pairwise_distances. When p = 1, this is equivalent to using manhattan_distance (l1), and euclidean_distance (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n",
    "\n",
    "metric_params : dict, optional (default = None)\n",
    "\n",
    "Additional keyword arguments for the metric function.\n",
    "\n",
    "n_jobs : int, optional (default = 1)\n",
    "\n",
    "The number of parallel jobs to run for neighbors search. If -1, then the number of jobs is set to the number of CPU cores. Affects only kneighbors and kneighbors_graph methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advantages of k-nearest neighbors algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No assumptions about data — useful, for example, for nonlinear data\n",
    "\n",
    "Simple algorithm — to explain and understand/interpret\n",
    "\n",
    "High accuracy (relatively) — it is pretty high but not competitive in comparison to better supervised learning models\n",
    "\n",
    "Versatile — useful for classification or regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitations to k-nearest neighbors algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computationally expensive — because the algorithm stores all of the training data\n",
    "\n",
    "High memory requirement\n",
    "\n",
    "Stores all (or almost all) of the training data\n",
    "\n",
    "Prediction stage might be slow (with big N)\n",
    "\n",
    "Sensitive to irrelevant features and the scale of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
