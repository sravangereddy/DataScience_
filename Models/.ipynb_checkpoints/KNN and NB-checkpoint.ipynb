{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN CLASSIFIER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Table of contents\n",
    "    - Introduction to k-nearest neighbor classifier\n",
    "    - K- nearest neighbor classification step by step procedure.\n",
    "        - Knn applications<br>\n",
    "        - Parameters <br>\n",
    "    - How to choose the k value<br>\n",
    "    - K-Nearest Neighbor case study<br>\n",
    "    - Advantages of k-nearest neighbors algorithm<br>\n",
    "    - Limitations to k-nearest neighbors algorithm<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to k-nearest neighbor classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-Nearest Neighbors is one of the most basic yet essential classification algorithms in Machine Learning. It belongs to the supervised learning domain and finds intense application in pattern recognition, data mining and intrusion detection.\n",
    "\n",
    "It is widely disposable in real-life scenarios since it is non-parametric, meaning, it does not make any underlying assumptions about the distribution of data (as opposed to other algorithms such as GMM, which assume a Gaussian distribution of the given data).\n",
    "\n",
    "The simple version of the K-nearest neighbor classifier algorithms is to <b>predict the target label by finding the nearest neighbor class</b>. The closest class will be identified using the distance measures like Euclidean distance.\n",
    "\n",
    "<b>KNN is a non-parametric, lazy learning algorithm.</b>\n",
    "\n",
    "When we say a technique is <b>non-parametric , it means that it does not make any assumptions on the underlying data distribution</b>. In other words, the model structure is determined from the data. If you think about it, it’s pretty useful, because in the “real world”, most of the data does not obey the typical theoretical assumptions made (as in linear regression models, for example). \n",
    "\n",
    "KNN is also a <b>lazy algorithm</b> (as opposed to an eager algorithm). What this means is that it does not use the training data points to do any generalization. <b>In other words, there is no explicit training phase or it is very minimal.</b> This also means that the training phase is pretty fast . \n",
    "\n",
    "<i>Lack of generalization means that KNN keeps all the training data. To be more exact, all (or most) the training data is needed during the testing phase.</i>\n",
    "\n",
    "KNN Algorithm is based on <b>feature similarity: How closely out-of-sample features resemble our training set determines how we classify a given data point.</b>\n",
    "\n",
    "KNN can be used for classification — the output is a class membership (predicts a class — a discrete value). <b>An object is classified by a majority vote of its neighbors, with the object being assigned to the class most common among its k nearest neighbors. It can also be used for regression — output is the value for the object (predicts continuous values). This value is the average (or median) of the values of its k nearest neighbors.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Knn applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Credit ratings — collecting financial characteristics vs. comparing people with similar financial features to a database. By the very nature of a credit rating, people who have similar financial details would be given similar credit ratings. Therefore, they would like to be able to use this existing database to predict a new customer’s credit rating, without having to perform all the calculations.\n",
    "\n",
    "Should the bank give a loan to an individual? Would an individual default on his or her loan? Is that person closer in characteristics to people who defaulted or did not default on their loans?\n",
    "\n",
    "In political science — classing a potential voter to a “will vote” or “will not vote”, or to “vote Democrat” or “vote Republican”.\n",
    "\n",
    "More advance examples could include handwriting detection (like OCR), image recognition and even video recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to choose the k value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training error rate and the validation error rate are two parameters we need to access on different K-value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=10)\n",
    "train_val_df = X_train.merge(y_train,left_index=True,right_index=True)\n",
    "# train_val_df = fn_upsample(train_val_df_,target_)\n",
    "train_val_df.reset_index(inplace=True)\n",
    "train_val_df\n",
    "neighbours = np.arange(1,50,11)\n",
    "neighbours = np.append(neighbours,np.arange(51,200,11))\n",
    "precision_train=[]\n",
    "recall_train=[]\n",
    "precision_val=[]\n",
    "recall_val=[]\n",
    "for neighbour in neighbours:\n",
    "    knn_model = KNeighborsClassifier(n_neighbors=neighbour,p=1)\n",
    "    precision_train_=[]\n",
    "    recall_train_=[]\n",
    "    precision_val_=[]\n",
    "    recall_val_=[]\n",
    "    for train,val in kf.split(train_val_df):\n",
    "        knn_model.fit(train_val_df.loc[train,input_var],train_val_df.loc[train,target_])\n",
    "        \n",
    "        knn_predictions_train = knn_model.predict(train_val_df.loc[train,input_var])\n",
    "        knn_predictions_val = knn_model.predict(train_val_df.loc[val,input_var])\n",
    "        \n",
    "        knn_classificaiton_report_train,conufsion_matrix_train=fn_get_classification_report(train_val_df.loc[train,target_],\n",
    "                                                                                            knn_predictions_train)\n",
    "        knn_classificaiton_report_val,conufsion_matrix_val=fn_get_classification_report(train_val_df.loc[val,target_],\n",
    "                                                                                            knn_predictions_val)\n",
    "\n",
    "        metrics_train = fn_get_metrics(conufsion_matrix_train)\n",
    "        metrics_val = fn_get_metrics(conufsion_matrix_val)\n",
    "        \n",
    "        precision_train_.append(metrics_train['5 Precision'])\n",
    "        recall_train_.append(metrics_train['6 Recall'])\n",
    "        \n",
    "        precision_val_.append(metrics_val['5 Precision'])\n",
    "        recall_val_.append(metrics_val['6 Recall'])\n",
    "    \n",
    "    precision_train.append(metrics_train['5 Precision'])\n",
    "    recall_train.append(metrics_train['6 Recall'])\n",
    "        \n",
    "    precision_val.append(metrics_val['5 Precision'])\n",
    "    recall_val.append(metrics_val['6 Recall'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(13.5, 8.5))\n",
    "plt.plot(neighbours,precision_train,label='Train')\n",
    "plt.plot(neighbours,precision_val,label='Validation')\n",
    "plt.title('Learning Curve - Precison')\n",
    "plt.xlabel('Neighbours')\n",
    "plt.ylabel('Precision')\n",
    "plt.grid(linestyle='-', linewidth=0.5)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/LearningCurve.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After applying cross validation on both training and validaton datasets, plotting the precison for each value helps to choose the right K Value. In the above graph, at K=1, the training dataset has a precison of 1 where as on the validation set, the precision was close to 0. This is where our model has over fit, also known as, high variance. As K increases, the precision on validation data set too increases and at some point the value recahes a threshold. This value is the opitimum value of K. In the case, choosing a K value of 30 gives the right results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-NN Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class sklearn.neighbors.NearestNeighbors(n_neighbors=5, radius=1.0, algorithm=’auto’, leaf_size=30, metric=’minkowski’, p=2, metric_params=None, n_jobs=1, **kwargs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n_neighbors : int, optional (default = 5)\n",
    "\n",
    "Number of neighbors to use by default for kneighbors queries.\n",
    "\n",
    "radius : float, optional (default = 1.0)\n",
    "\n",
    "Range of parameter space to use by default for radius_neighbors queries.\n",
    "\n",
    "algorithm : {‘auto’, ‘ball_tree’, ‘kd_tree’, ‘brute’}, optional\n",
    "\n",
    "Algorithm used to compute the nearest neighbors:\n",
    "\n",
    "    ‘ball_tree’ will use BallTree\n",
    "    ‘kd_tree’ will use KDTree\n",
    "    ‘brute’ will use a brute-force search.\n",
    "    ‘auto’ will attempt to decide the most appropriate algorithm based on the values passed to fit method.\n",
    "    Note: fitting on sparse input will override the setting of this parameter, using brute force.\n",
    "\n",
    "leaf_size : int, optional (default = 30)\n",
    "\n",
    "    Leaf size passed to BallTree or KDTree. This can affect the speed of the construction and query, as well as the memory required to store the tree. The optimal value depends on the nature of the problem.\n",
    "\n",
    "metric : string or callable, default ‘minkowski’\n",
    "\n",
    "    metric to use for distance computation. Any metric from scikit-learn or scipy.spatial.distance can be used.\n",
    "\n",
    "    If metric is a callable function, it is called on each pair of instances (rows) and the resulting value recorded. The callable should take two arrays as input and return one value indicating the distance between them. This works for Scipy’s metrics, but is less efficient than passing the metric name as a string.\n",
    "\n",
    "    Distance matrices are not supported.\n",
    "\n",
    "    Valid values for metric are:\n",
    "\n",
    "    from scikit-learn: [‘cityblock’, ‘cosine’, ‘euclidean’, ‘l1’, ‘l2’, ‘manhattan’]\n",
    "    from scipy.spatial.distance: [‘braycurtis’, ‘canberra’, ‘chebyshev’, ‘correlation’, ‘dice’, ‘hamming’, ‘jaccard’, ‘kulsinski’, ‘mahalanobis’, ‘matching’, ‘minkowski’, ‘rogerstanimoto’, ‘russellrao’, ‘seuclidean’, ‘sokalmichener’, ‘sokalsneath’, ‘sqeuclidean’, ‘yule’]\n",
    "    See the documentation for scipy.spatial.distance for details on these metrics.\n",
    "\n",
    "p : integer, optional (default = 2)\n",
    "\n",
    "    Parameter for the Minkowski metric from sklearn.metrics.pairwise.pairwise_distances. When p = 1, this is equivalent to using manhattan_distance (l1), and euclidean_distance (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n",
    "\n",
    "metric_params : dict, optional (default = None)\n",
    "\n",
    "Additional keyword arguments for the metric function.\n",
    "\n",
    "n_jobs : int, optional (default = 1)\n",
    "\n",
    "The number of parallel jobs to run for neighbors search. If -1, then the number of jobs is set to the number of CPU cores. Affects only kneighbors and kneighbors_graph methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advantages of k-nearest neighbors algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No assumptions about data — useful, for example, for nonlinear data\n",
    "\n",
    "Simple algorithm — to explain and understand/interpret\n",
    "\n",
    "High accuracy (relatively) — it is pretty high but not competitive in comparison to better supervised learning models\n",
    "\n",
    "Versatile — useful for classification or regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitations to k-nearest neighbors algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computationally expensive — because the algorithm stores all of the training data\n",
    "\n",
    "High memory requirement\n",
    "\n",
    "Stores all (or almost all) of the training data\n",
    "\n",
    "Prediction stage might be slow (with big N)\n",
    "\n",
    "Sensitive to irrelevant features and the scale of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What is Naive Bayes algorithm?<br>\n",
    "- Naive Bayes Algorithms in Python<br>\n",
    "- Advantages of using Naive Bayes?<br>\n",
    "- Limitations of using Naive Bayes\n",
    "- Applications of Naive Bayes Algorithm<br>\n",
    "- Steps to build a basic Naive Bayes Model in Python<br>\n",
    "- Tips to improve the power of Naive Bayes Model<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a classification technique based on Bayes’ Theorem with an assumption of independence among predictors. In simple terms, a Naive Bayes classifier assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature. For example, a fruit may be considered to be an apple if it is red, round, and about 3 inches in diameter. Even if these features depend on each other or upon the existence of the other features, all of these properties independently contribute to the probability that this fruit is an apple and that is why it is known as ‘Naive’.\n",
    "\n",
    "Naive Bayes model is easy to build and particularly useful for very large data sets. Along with simplicity, Naive Bayes is known to outperform even highly sophisticated classification methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/NB.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scikit learn (python library) will  here to build a Naive Bayes model in Python. There are three types of Naive Bayes model under scikit learn library:\n",
    "\n",
    "Gaussian: It is used in classification and it assumes that features follow a normal distribution.\n",
    "\n",
    "Multinomial: It is used for discrete counts. For example, let’s say,  we have a text classification problem. Here we can consider bernoulli trials which is one step further and instead of “word occurring in the document”, we have “count how often word occurs in the document”, you can think of it as “number of times outcome number x_i is observed over the n trials”.\n",
    "\n",
    "Bernoulli: The binomial model is useful if your feature vectors are binary (i.e. zeros and ones). One application would be text classification with ‘bag of words’ model where the 1s & 0s are “word occurs in the document” and “word does not occur in the document” respectively.\n",
    "\n",
    "Based on your data set, you can choose any of above discussed model. Below is the example of Gaussian model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Gaussian NB</b>class sklearn.naive_bayes.GaussianNB(priors=None)\n",
    "\n",
    "priors : array-like, shape (n_classes,)\n",
    "\n",
    "Prior probabilities of the classes. If specified the priors are not adjusted according to the data.\n",
    "\n",
    "<b> Multinomial NB </b>\n",
    "class sklearn.naive_bayes.MultinomialNB(alpha=1.0, fit_prior=True, class_prior=None)\n",
    "\n",
    "alpha : float, optional (default=1.0)\n",
    "\n",
    "    Additive (Laplace/Lidstone) smoothing parameter (0 for no smoothing).\n",
    "\n",
    "fit_prior : boolean, optional (default=True)\n",
    "\n",
    "    Whether to learn class prior probabilities or not. If false, a uniform prior will be used.\n",
    "\n",
    "class_prior : array-like, size (n_classes,), optional (default=None)\n",
    "\n",
    "    Prior probabilities of the classes. If specified the priors are not adjusted according to the data.\n",
    "\n",
    "<b>BernoulliNB</b>class sklearn.naive_bayes.BernoulliNB(alpha=1.0, binarize=0.0, fit_prior=True, class_prior=None)\n",
    "\n",
    "alpha : float, optional (default=1.0)\n",
    "\n",
    "    Additive (Laplace/Lidstone) smoothing parameter (0 for no smoothing).\n",
    "\n",
    "binarize : float or None, optional (default=0.0)\n",
    "\n",
    "    Threshold for binarizing (mapping to booleans) of sample features. If None, input is presumed to already consist of binary vectors.\n",
    "\n",
    "fit_prior : boolean, optional (default=True)\n",
    "\n",
    "    Whether to learn class prior probabilities or not. If false, a uniform prior will be used.\n",
    "\n",
    "class_prior : array-like, size=[n_classes,], optional (default=None)\n",
    "\n",
    "    Prior probabilities of the classes. If specified the priors are not adjusted according to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "berNB_model = BernoulliNB(alpha=1000)\n",
    "berNB_model.fit(X_[input_var],X_[target_])\n",
    "berNB_predictions_test = berNB_model.predict(X_test[input_var])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Alpha - Laplace Smoothing : </b>\n",
    "\n",
    "In text processing, it's likely that in test dataset, there might be words which were not encountered in training dataset. In such cases, Naive Bayes will assign a probabilty of 0. In such cases, the entire probabilty estimate of the record will be 0. To avoid this, a smoothing parameter called Laplace Smoothing, indicated by alpha is prvoded which is added to each value of the probabilty obtained. By doing this, for a new word, even though the probabilty is 0, adding the smoothong paramter will make the probability non zero.\n",
    "\n",
    "One more bigger advantage of using smoothing parameter is that, in training dataset the probabilty of a regualr word occuring is very very high compared to words whcih seldom occur. In such secanrios, NB assigns least probbailty to workds that don't occur frequently, thus assigning a very less probabilty. To overcome this, we add the smoothing parameter. By adding a smoothing parameter for a small value, the probbailty value will increase slightly. By providing high values of alpha, the probabilty tends to got 0.5. This way, we are trying to assign almost equal pribabilties for words which occur less frequently. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advantages of NB algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It is easy and fast to predict class of test data set. \n",
    "- It also perform well in multi class prediction\n",
    "- When assumption of independence holds, a Naive Bayes classifier performs better compare to other models like logistic regression and you need less training data.\n",
    "- It perform well in case of categorical input variables compared to numerical variable(s). For numerical variable, normal distribution is assumed (bell curve, which is a strong assumption).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitations of using Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If categorical variable has a category (in test data set), which was not observed in training data set, then model will assign a 0 (zero) probability and will be unable to make a prediction. This is often known as “Zero Frequency”. To solve this, we can use the smoothing technique. One of the simplest smoothing techniques is called Laplace estimation.\n",
    "- On the other side naive Bayes is also known as a bad estimator, so the probability outputs from predict_proba are not to be taken too seriously.\n",
    "- Another limitation of Naive Bayes is the assumption of independent predictors. In real life, it is almost impossible that we get a set of predictors which are completely independent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applications of Naive Bayes Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Real time Prediction: Naive Bayes is an eager learning classifier and it is sure fast. Thus, it could be used for making predictions in real time.\n",
    "- Multi class Prediction: This algorithm is also well known for multi class prediction feature. Here we can predict the probability of multiple classes of target variable.\n",
    "- Text classification/ Spam Filtering/ Sentiment Analysis: Naive Bayes classifiers mostly used in text classification (due to better result in multi class problems and independence rule) have higher success rate as compared to other algorithms. As a result, it is widely used in Spam filtering (identify spam e-mail) and Sentiment Analysis (in social media analysis, to identify positive and negative customer sentiments)\n",
    "- Recommendation System: Naive Bayes Classifier and Collaborative Filtering together builds a Recommendation System that uses machine learning and data mining techniques to filter unseen information and predict whether a user would like a given resource or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
